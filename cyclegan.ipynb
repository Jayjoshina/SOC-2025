{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4haQsfqe1TS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3),\n",
        "            nn.InstanceNorm2d(dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResNetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_blocks=6):\n",
        "        super().__init__()\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features *= 2\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResNetBlock(in_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features //= 2\n",
        "\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(64, output_nc, 7),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "VU6Y6D6mf8Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc):\n",
        "        super().__init__()\n",
        "        model = [\n",
        "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(3):\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 4, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features *= 2\n",
        "\n",
        "        model += [nn.Conv2d(in_features, 1, 4, padding=1)]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "oMwRY8fgf-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gan_loss(pred, target_is_real):\n",
        "    target = torch.ones_like(pred) if target_is_real else torch.zeros_like(pred)\n",
        "    return F.mse_loss(pred, target)\n",
        "\n",
        "def cycle_loss(reconstructed, original):\n",
        "    return F.l1_loss(reconstructed, original)\n",
        "\n",
        "def identity_loss(same, original):\n",
        "    return F.l1_loss(same, original)\n"
      ],
      "metadata": {
        "id": "XB-W3tQFgC1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize models\n",
        "G_AB = ResNetGenerator(3, 3).to(device)\n",
        "G_BA = ResNetGenerator(3, 3).to(device)\n",
        "D_A = Discriminator(3).to(device)\n",
        "D_B = Discriminator(3).to(device)\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = torch.optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "d_a_optimizer = torch.optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d_b_optimizer = torch.optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Load datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "dataset_A = datasets.ImageFolder(\"\", transform=transform)\n",
        "dataset_B = datasets.ImageFolder(\"\", transform=transform)\n",
        "loader_A = DataLoader(dataset_A, batch_size=1, shuffle=True)\n",
        "loader_B = DataLoader(dataset_B, batch_size=1, shuffle=True)\n",
        "\n",
        "# Training\n",
        "lambda_cyc = 10\n",
        "lambda_id = 5\n",
        "\n",
        "for epoch in range(100):\n",
        "    for real_A, _ in loader_A:\n",
        "        real_B, _ = next(iter(loader_B))\n",
        "\n",
        "        real_A = real_A.to(device)\n",
        "        real_B = real_B.to(device)\n",
        "\n",
        "        #Train Generators\n",
        "        g_optimizer.zero_grad()\n",
        "\n",
        "        fake_B = G_AB(real_A)\n",
        "        rec_A = G_BA(fake_B)\n",
        "\n",
        "        fake_A = G_BA(real_B)\n",
        "        rec_B = G_AB(fake_A)\n",
        "\n",
        "        # Identity loss\n",
        "        idt_A = G_BA(real_A)\n",
        "        idt_B = G_AB(real_B)\n",
        "\n",
        "        loss_id = identity_loss(idt_A, real_A) + identity_loss(idt_B, real_B)\n",
        "\n",
        "        # GAN loss\n",
        "        loss_gan_AB = gan_loss(D_B(fake_B), True)\n",
        "        loss_gan_BA = gan_loss(D_A(fake_A), True)\n",
        "\n",
        "        # Cycle loss\n",
        "        loss_cyc = cycle_loss(rec_A, real_A) + cycle_loss(rec_B, real_B)\n",
        "\n",
        "        loss_G = loss_gan_AB + loss_gan_BA + lambda_cyc * loss_cyc + lambda_id * loss_id\n",
        "        loss_G.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Train Discriminator A\n",
        "        d_a_optimizer.zero_grad()\n",
        "        loss_real = gan_loss(D_A(real_A), True)\n",
        "        loss_fake = gan_loss(D_A(fake_A.detach()), False)\n",
        "        loss_D_A = (loss_real + loss_fake) * 0.5\n",
        "        loss_D_A.backward()\n",
        "        d_a_optimizer.step()\n",
        "\n",
        "        #Train Discriminator B\n",
        "        d_b_optimizer.zero_grad()\n",
        "        loss_real = gan_loss(D_B(real_B), True)\n",
        "        loss_fake = gan_loss(D_B(fake_B.detach()), False)\n",
        "        loss_D_B = (loss_real + loss_fake) * 0.5\n",
        "        loss_D_B.backward()\n",
        "        d_b_optimizer.step()\n"
      ],
      "metadata": {
        "id": "dci4IKKDgF7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "G_AB.eval()\n",
        "with torch.no_grad():\n",
        "    for real_A, _ in loader_A:\n",
        "        real_A = real_A.to(device)\n",
        "        fake_B = G_AB(real_A)\n",
        "        save_image((fake_B + 1) / 2, 'sample_output.png')  # de-normalize and save\n",
        "        break\n"
      ],
      "metadata": {
        "id": "Q2f81KTFgOl-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}